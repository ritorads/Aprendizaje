#################################################################################
# Calcular la peligrosidad
Data_A_scaled$peligrosidad <- 0.5 * Data_A_scaled$Y1 + 0.3 * Data_A_scaled$NumberOfTimes + 0.2 * Data_A_scaled$WeaponType
train_data <- Data_A_scaled[indices, ]
test_data <- Data_A_scaled[-indices, ]
# Entrenar la red neuronal
formula_peligrosidad <- as.formula("peligrosidad ~ Y1 + Gender + Penal + NumberOfTimes + WeaponType")
modeloM2 <- neuralnet(formula_peligrosidad,
data = train_data,
hidden = c(3, 2),
linear.output = TRUE)
pred_Peligrosidad <- predict(modeloM2, test_data)
mse_m2 <- mean((test_data$peligrosidad - pred_Peligrosidad)^2)
print(paste("MSE del Modelo M2:", round(mse_m2, 4)))
plot(modeloM2)
# Insertar la columna Y2 en la posición 10 de Data_A_scaled
Data_A_scaled <- cbind(Data_A_scaled[, 1:9], Y2 = predict(modeloM2, Data_A_scaled), Data_A_scaled[, 10:ncol(Data_A_scaled)])
data_M3 <- data.frame(
Y2 = Data_A_scaled$Y2,
Data_A_scaled$Comuna,
Data_A_scaled$Nivel,
Data_A_scaled$Reincidente
)
#################################################################################
######################  Modelo M3  ###############################################
#################################################################################
Data_A_scaled$Barrio_seguro <- 0.5 * Data_A_scaled$Nivel + 0.3 * Data_A_scaled$Y2 + 0.2 * Data_A_scaled$Reincidente
train_data <- Data_A_scaled[indices, ]
test_data <- Data_A_scaled[-indices, ]
# Entrenar la red neuronal
formula_barrio_seguro <- as.formula("Barrio_seguro ~ Y2 + Comuna + Nivel + Reincidente")
modeloM3 <- neuralnet(formula_barrio_seguro,
data = train_data,
hidden = c(3, 2),
linear.output = TRUE)
pred_Barrio_seguro <- predict(modeloM3, test_data)
mse_m3 <- mean((test_data$Barrio_seguro - pred_Barrio_seguro)^2)
print(paste("MSE del Modelo M3:", round(mse_m3, 4)))
plot(modeloM3)
Data_A_scaled$Y3 <- predict(modeloM3, Data_A_scaled)
Data_A_scaled <- cbind(Data_A_scaled[, 1:13], Y3 = predict(modeloM3, Data_A_scaled), Data_A_scaled[, 14:ncol(Data_A_scaled)])
# dataframe de los errores de cada modelo
df_errores <- data.frame(
Modelo = c("M1", "M2", "M3"),
MSE = c(mse_m1, mse_m2, mse_m3)
)
# Desescalar el dataset completo
Data_A_original <- Data_A_scaled
# Eliminar las últimas 2 columnas
# Clasificar en nivel 0 - 4
clasificar <- function(x) {
if (x < 0.25) {
return(0)
} else if (x < 0.50 & x >= 0.25) {
return(1)
} else if ( x < 0.75 & x >= 0.50) {
return(2)
} else if (x >= 0.75){
return(3)
}
}
Data_A_original$Y1 <- sapply(Data_A_original$Y1, clasificar)
Data_A_original$Y2 <- sapply(Data_A_original$Y2, clasificar)
Data_A_original$Y3 <- sapply(Data_A_original$Y3, clasificar)
# Mostrar las primeras filas del dataset desescalado
head(Data_A_original)
Data_A_original <- Data_A_original[, -((ncol(Data_A_original)-2):ncol(Data_A_original))]
Testeo <- function(individuos_pruebas) {
# Definir dos individuos de prueba en un dataframe
# Escalar los datos de los individuos de prueba
individuos_pruebas <- as.data.frame(scale(individuos_prueba))
# Predicción con el Modelo M1 para 'Murder'
individuos_pruebas$Y1 <- predict(modeloM1, individuos_pruebas)
# Predicción con el Modelo M2 para 'peligrosidad'
individuos_pruebas$Y2 <- predict(modeloM2, individuos_pruebas)
# Predicción con el Modelo M3 para 'Barrio_seguro'
individuos_pruebas$Y3 <- predict(modeloM3, individuos_pruebas)
individuos_prueba$Y1 <- sapply(individuos_pruebas$Y1, clasificar)
individuos_prueba$Y2 <- sapply(individuos_pruebas$Y2, clasificar)
individuos_prueba$Y3 <- sapply(individuos_pruebas$Y3, clasificar)
# Mostrar el dataframe con las predicciones
print(individuos_prueba)
}
# Definir dos individuos de prueba en un dataframe
individuos_prueba <- data.frame(
Assault = c(10, 5, 3, 0),
UrbanPop = c(70, 5, 5000, 0),
Rape = c(5, 5, 0, 0),
Gender = c(2, 1, 1, 0),
Penal = c(3, 1, 1, 0),
NumberOfTimes = c(3, 2, 1, 0),
WeaponType = c(2, 1, 0, 0),
Comuna = c(3, 2, 1, 3),
Nivel = c(1, 1, 0, 1),
Reincidente = c(2, 1, 0, 0)
)
# Llamar a la función Testeo
Testeo(individuos_prueba)
# Asegúrate de que 'Data_A_scaled' contiene las columnas 'Comuna' y 'peligrosidad'
Data_A_original$Comuna <- Data_A_original$Comuna  # Asegurar que 'Comuna' está presente
# Paso 1: Ya deberías tener las puntuaciones individuales de peligrosidad en 'Data_A_scaled$peligrosidad'
# Paso 2: Calcular la peligrosidad promedio por comuna
peligrosidad_por_comuna <- aggregate(Y3 ~ Comuna, data = Data_A_original, FUN = mean)
# Paso 3: Clasificar las comunas en niveles de peligrosidad
# Definir los puntos de corte (por ejemplo, utilizando cuantiles)
cuantiles <- quantile(peligrosidad_por_comuna$Y3, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE)
# Asegurarse de que los breaks sean únicos
cuantiles <- unique(cuantiles)
# Clasificar las comunas
peligrosidad_por_comuna$Nivel_Peligrosidad <- cut(
peligrosidad_por_comuna$Y3,
breaks = cuantiles,
labels = c("Bajo", "Medio", "Alto"),
include.lowest = TRUE
)
# Mostrar el resumen
print(peligrosidad_por_comuna)
# Gráfico de barras
ggplot(peligrosidad_por_comuna, aes(x = Comuna, y = Y3, fill = Nivel_Peligrosidad)) +
geom_bar(stat = "identity") +
xlab("Comuna") +
ylab("Peligrosidad Promedio") +
ggtitle("Niveles de Peligrosidad por Comuna") +
theme_minimal()
# Cargar las librerías necesarias
library(neuralnet)
library(corrplot)
library(NeuralNetTools)
library(dplyr)
library(ggplot2)
# Carga del dataset USArrests
Data_A <- USArrests
# Generar variables categóricas adicionales para M2 y M3
Gender <- as.factor(sample(c("Male", "Female", "no-binario"), 50, replace = TRUE))
Penal <- as.factor(sample(c("Colina 1", "Santiago 1", "Rancagua", "Punta Peuco"), 50, replace = TRUE))
NumberOfTimes <- as.factor(sample(1:5, 50, replace = TRUE))
WeaponType <- as.factor(sample(c("Knife", "Gun", "Grenade"), 50, replace = TRUE))
Comuna <- as.factor(sample(c("La Reina", "Santiago", "La Cisterna", "Macul", "Peñalolén"), 50, replace = TRUE))
Nivel <- as.factor(sample(c("Bajo", "Alto"), 50, replace = TRUE))
Reincidente <- as.factor(sample(c("No", "Si"), 50, replace = TRUE))
# Convertir factores a numéricos para el uso en redes neuronales
Gender_num <- as.numeric(Gender)
Penal_num <- as.numeric(Penal)
NumberOfTimes_num <- as.numeric(NumberOfTimes)
WeaponType_num <- as.numeric(WeaponType)
Comuna_num <- as.numeric(Comuna)
Nivel_num <- as.numeric(Nivel)
Reincidente_num <- as.numeric(Reincidente)
# Crear el data frame con las variables generadas
Data_A_full <- data.frame(
Murder = Data_A$Murder,
Assault = Data_A$Assault,
UrbanPop = Data_A$UrbanPop,
Rape = Data_A$Rape,
Gender = Gender_num,
Penal = Penal_num,
NumberOfTimes = NumberOfTimes_num,
WeaponType = WeaponType_num,
Comuna = Comuna_num,
Nivel = Nivel_num,
Reincidente = Reincidente_num
)
# Guardar medias y desviaciones estándar para cada columna
medias <- sapply(Data_A_full, mean)
desviaciones <- sapply(Data_A_full, sd)
Data_A_scaled <- as.data.frame(scale(Data_A_full))
# Train / test
set.seed(123)  # Para reproducibilidad
indices <- sample(1:nrow(Data_A_scaled), size = 0.8 * nrow(Data_A_scaled))
train_data <- Data_A_scaled[indices, ]
test_data <- Data_A_scaled[-indices, ]
#################################################################################
######################  Modelo M1  ##############################################
#################################################################################
# Definir la fórmula del modelo
formula_m1 <- as.formula("Murder ~ Assault + UrbanPop + Rape")
modeloM1 <- neuralnet(formula_m1,
data = train_data,
hidden = c(3,2),
linear.output = TRUE)
pred_Murder <- predict(modeloM1, test_data)
mse_m1 <- mean((test_data$Murder - pred_Murder)^2)
print(paste("MSE del Modelo M1:", round(mse_m1, 4)))
plot(modeloM1)
# Predict del conjunto
Data_A_scaled <- cbind(Data_A_scaled[, 1:4], Y1 = predict(modeloM1, Data_A_scaled), Data_A_scaled[, 5:ncol(Data_A_scaled)])
# Crear un dataframe con las columnas especificadas y nombres de columnas
data_M2 <- data.frame(
Y1 = Data_A_scaled$Y1,
Gender = Data_A_scaled$Gender,
Penal = Data_A_scaled$Penal,
NumberOfTimes = Data_A_scaled$NumberOfTimes,
WeaponType = Data_A_scaled$WeaponType
)
#################################################################################
######################  Modelo M2  ###############################################
#################################################################################
# Calcular la peligrosidad
Data_A_scaled$peligrosidad <- 0.5 * Data_A_scaled$Y1 + 0.3 * Data_A_scaled$NumberOfTimes + 0.2 * Data_A_scaled$WeaponType
train_data <- Data_A_scaled[indices, ]
test_data <- Data_A_scaled[-indices, ]
# Entrenar la red neuronal
formula_peligrosidad <- as.formula("peligrosidad ~ Y1 + Gender + Penal + NumberOfTimes + WeaponType")
modeloM2 <- neuralnet(formula_peligrosidad,
data = train_data,
hidden = c(3, 2),
linear.output = TRUE)
pred_Peligrosidad <- predict(modeloM2, test_data)
mse_m2 <- mean((test_data$peligrosidad - pred_Peligrosidad)^2)
print(paste("MSE del Modelo M2:", round(mse_m2, 4)))
plot(modeloM2)
# Insertar la columna Y2 en la posición 10 de Data_A_scaled
Data_A_scaled <- cbind(Data_A_scaled[, 1:9], Y2 = predict(modeloM2, Data_A_scaled), Data_A_scaled[, 10:ncol(Data_A_scaled)])
data_M3 <- data.frame(
Y2 = Data_A_scaled$Y2,
Data_A_scaled$Comuna,
Data_A_scaled$Nivel,
Data_A_scaled$Reincidente
)
#################################################################################
######################  Modelo M3  ###############################################
#################################################################################
Data_A_scaled$Barrio_seguro <- 0.5 * Data_A_scaled$Nivel + 0.3 * Data_A_scaled$Y2 + 0.2 * Data_A_scaled$Reincidente
train_data <- Data_A_scaled[indices, ]
test_data <- Data_A_scaled[-indices, ]
# Entrenar la red neuronal
formula_barrio_seguro <- as.formula("Barrio_seguro ~ Y2 + Comuna + Nivel + Reincidente")
modeloM3 <- neuralnet(formula_barrio_seguro,
data = train_data,
hidden = c(3, 2),
linear.output = TRUE)
pred_Barrio_seguro <- predict(modeloM3, test_data)
mse_m3 <- mean((test_data$Barrio_seguro - pred_Barrio_seguro)^2)
print(paste("MSE del Modelo M3:", round(mse_m3, 4)))
plot(modeloM3)
Data_A_scaled$Y3 <- predict(modeloM3, Data_A_scaled)
Data_A_scaled <- cbind(Data_A_scaled[, 1:13], Y3 = predict(modeloM3, Data_A_scaled), Data_A_scaled[, 14:ncol(Data_A_scaled)])
# dataframe de los errores de cada modelo
df_errores <- data.frame(
Modelo = c("M1", "M2", "M3"),
MSE = c(mse_m1, mse_m2, mse_m3)
)
# Desescalar el dataset completo
Data_A_original <- as.data.frame(Data_A_scaled)
for (col in names(Data_A_full)) {
Data_A_original[[col]] <- Data_A_scaled[, col] * desviaciones[col] + medias[col]
}
# Eliminar las últimas 2 columnas
# Clasificar en nivel 0 - 4
clasificar <- function(x) {
if (x < 0.25) {
return(0)
} else if (x < 0.50 & x >= 0.25) {
return(1)
} else if ( x < 0.75 & x >= 0.50) {
return(2)
} else if (x >= 0.75){
return(3)
}
}
Data_A_original$Y1 <- sapply(Data_A_original$Y1, clasificar)
Data_A_original$Y2 <- sapply(Data_A_original$Y2, clasificar)
Data_A_original$Y3 <- sapply(Data_A_original$Y3, clasificar)
# Mostrar las primeras filas del dataset desescalado
head(Data_A_original)
Data_A_original <- Data_A_original[, -((ncol(Data_A_original)-2):ncol(Data_A_original))]
Testeo <- function(individuos_pruebas) {
# Definir dos individuos de prueba en un dataframe
# Escalar los datos de los individuos de prueba
individuos_pruebas <- as.data.frame(scale(individuos_prueba))
# Predicción con el Modelo M1 para 'Murder'
individuos_pruebas$Y1 <- predict(modeloM1, individuos_pruebas)
# Predicción con el Modelo M2 para 'peligrosidad'
individuos_pruebas$Y2 <- predict(modeloM2, individuos_pruebas)
# Predicción con el Modelo M3 para 'Barrio_seguro'
individuos_pruebas$Y3 <- predict(modeloM3, individuos_pruebas)
individuos_prueba$Y1 <- sapply(individuos_pruebas$Y1, clasificar)
individuos_prueba$Y2 <- sapply(individuos_pruebas$Y2, clasificar)
individuos_prueba$Y3 <- sapply(individuos_pruebas$Y3, clasificar)
# Mostrar el dataframe con las predicciones
print(individuos_prueba)
}
# Definir dos individuos de prueba en un dataframe
individuos_prueba <- data.frame(
Assault = c(10, 5, 3, 0),
UrbanPop = c(70, 5, 5000, 0),
Rape = c(5, 5, 0, 0),
Gender = c(2, 1, 1, 0),
Penal = c(3, 1, 1, 0),
NumberOfTimes = c(3, 2, 1, 0),
WeaponType = c(2, 1, 0, 0),
Comuna = c(3, 2, 1, 3),
Nivel = c(1, 1, 0, 1),
Reincidente = c(2, 1, 0, 0)
)
# Llamar a la función Testeo
Testeo(individuos_prueba)
# Asegúrate de que 'Data_A_scaled' contiene las columnas 'Comuna' y 'peligrosidad'
Data_A_original$Comuna <- Data_A_original$Comuna  # Asegurar que 'Comuna' está presente
# Paso 1: Ya deberías tener las puntuaciones individuales de peligrosidad en 'Data_A_scaled$peligrosidad'
# Paso 2: Calcular la peligrosidad promedio por comuna
peligrosidad_por_comuna <- aggregate(Y3 ~ Comuna, data = Data_A_original, FUN = mean)
# Paso 3: Clasificar las comunas en niveles de peligrosidad
# Definir los puntos de corte (por ejemplo, utilizando cuantiles)
cuantiles <- quantile(peligrosidad_por_comuna$Y3, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE)
# Asegurarse de que los breaks sean únicos
cuantiles <- unique(cuantiles)
# Clasificar las comunas
peligrosidad_por_comuna$Nivel_Peligrosidad <- cut(
peligrosidad_por_comuna$Y3,
breaks = cuantiles,
labels = c("Bajo", "Medio", "Alto"),
include.lowest = TRUE
)
# Mostrar el resumen
print(peligrosidad_por_comuna)
# Gráfico de barras
ggplot(peligrosidad_por_comuna, aes(x = Comuna, y = Y3, fill = Nivel_Peligrosidad)) +
geom_bar(stat = "identity") +
xlab("Comuna") +
ylab("Peligrosidad Promedio") +
ggtitle("Niveles de Peligrosidad por Comuna") +
theme_minimal()
str(data)
summary(data)
data <- as.data.frame(lapply(data, Normalizar)
data <- read.csv("TARP.csv")
setwd("C:/Users/andre/OneDrive/Escritorio/GitHub/Aprendizaje/DataAnalisis/R")
data <- read.csv("TARP.csv")
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
data <- as.data.frame(lapply(data, Normalizar)
View(data)
View(data)
cor(data)
View(data)
data$Estado <- as.factor(data$Estado)
str(data)
summary(data)
cor(data)
data_norm <- Normalizar(data[, 1:14])
cor(data)
cor(data_norm)
View(data)
View(data_norm)
data$Estado <- ifelse(data$Estado == "Si", 1,
ifelse(data$Estado == "No", 0, NA))
View(data)
data <- read.csv("TARP.csv")
head(data)
data$Estado <- ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA))
data$Estado <- ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA))
View(data)
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
data$Estado <- ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA))
View(data)
str(data)
summary(data)
data_norm <- Normalizar(data[, 1:14])
cor(data_norm)
data_norm <- Normalizar(data[, 1:14])
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
data_norm <- Normalizar(data[, 1:14])
cor(data_norm)
View(data_norm)
data_scaled <- as.data.frame(lapply(data, Normalizar))
cor(data_norm)
View(data)
data <- read.csv("TARP.csv")
head(data)
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
data$Estado <- ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA))
str(data)
summary(data)
data_scaled <- as.data.frame(lapply(data, Normalizar))
View(data_scaled)
View(data)
data_scaled <- as.data.frame(lapply(data, Normalizar))
View(data_scaled)
data$Estado <- as.factor(data$Estado)
View(data)
data <- read.csv("TARP.csv")
head(data)
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
data$Estado <- as.factor(data$Estado)
View(data)
data$Estado <- as.factor(ifelse(data$Estado == "ON", 1, 0))
data$Estado <- as.factor(ifelse(data$Estado == "ON", 1, 0))
data$Estado <- as.factor(ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA)))
data <- read.csv("TARP.csv")
head(data)
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
data$Estado <- as.factor(ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA)))
View(data)
str(data)
summary(data)
# Seleccionar las columnas numéricas para normalizar (excluyendo 'Estado')
cols_to_normalize <- names(data)[sapply(data, is.numeric) & names(data) != "Estado"]
# Aplicar la función de normalización a las columnas seleccionadas
data_scaled <- data  # Crear una copia del dataframe original
data_scaled[cols_to_normalize] <- lapply(data[cols_to_normalize], Normalizar)
# Verificar los datos normalizados
head(data_scaled)
View(data_scaled)
View(data)
# Manejar valores faltantes
df <- na.omit(df)
# Seleccionar las columnas numéricas para normalizar (excluyendo 'Estado')
cols_to_normalize <- names(data)[sapply(data, is.numeric) & names(data) != "Estado"]
# Aplicar la función de normalización a las columnas seleccionadas
data_scaled <- data  # Crear una copia del dataframe original
data_scaled[cols_to_normalize] <- lapply(data[cols_to_normalize], Normalizar)
# Verificar los datos normalizados
head(data_scaled)
# Manejar valores faltantes
data <- na.omit(data)
data <- read.csv("TARP.csv")
head(data)
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
# Manejar valores faltantes
data <- na.omit(data)
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# Análisis Exploratorio de Datos
data$Estado <- as.factor(ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA)))
str(data)
summary(data)
# Seleccionar las columnas numéricas para normalizar (excluyendo 'Estado')
cols_to_normalize <- names(data)[sapply(data, is.numeric) & names(data) != "Estado"]
# Aplicar la función de normalización a las columnas seleccionadas
data_scaled <- data  # Crear una copia del dataframe original
data_scaled[cols_to_normalize] <- lapply(data[cols_to_normalize], Normalizar)
# Verificar los datos normalizados
head(data_scaled)
data_norm <- as.data.frame(lapply(data[, 1:14], Normalizar))
data <- read.csv("TARP.csv")
head(data)
colnames(data) <- c("Humedad del Suelo", "Temperatura", "Humedad del Suelo (Adicional)", "Tiempo",
"Temperatura del Aire (C)", "Velocidad del Viento (Km/h)", "Humedad del Aire (%)",
"Ráfagas de Viento (Km/h)", "Presión (KPa)", "pH", "Lluvia", "Nitrógeno (N)",
"Fósforo (P)", "Potasio (K)", "Estado"
)
# Manejar valores faltantes
data <- na.omit(data)
# Normalización
Normalizar <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
# Análisis Exploratorio de Datos
data$Estado <- as.factor(ifelse(data$Estado == "ON", 1,
ifelse(data$Estado == "OFF", 0, NA)))
str(data)
summary(data)
data_norm <- as.data.frame(lapply(data[, 1:14], Normalizar))
View(data_norm)
cor(data_norm)
corrplot(cor(data_norm), method = "number")
library(ggplot2)
library(corrplot)
library(neuralnet)
corrplot(cor(data_norm), method = "number")
